---
layout: post
title: lvs
date: 2024-03-26
tags: 
comments: true
author: lewx
---
# What？

**[LVS](http://www.linuxvirtualserver.org/whatis.html)**是建立在真实服务器集群上的高度可扩展和高可用性的服务器。服务器集群的架构**对用户是完全透明**的，用户与集群系统进行交互，**就好像它只是一个高性能的虚拟服务器**一样。

请求调度可以通过IP 负载均衡技术或应用程序级负载均衡技术实现。系统的可扩展性是通过在集群中透明地添加或删除节点来实现的。高可用性通过检测节点或守护进程故障并适当地重新配置系统来提供。

![img](http://www.linuxvirtualserver.org/VirtualServer.png)



# Why？

构造集群的几种方式：

- 基于DNS的负载均衡集群

- 基于Dispatcher 的负载均衡集群

  Dispatcher（也称为负载平衡器）可用于在集群中的服务器之间分配负载，以便服务器的并行服务可以在**单个 IP 地址（VIP）**上显示为虚拟服务，最终用户可以像单个服务器一样进行交互，而无需知道集群中的所有服务器。与基于 DNS 的负载均衡相比，调度程序可以按照更细的粒度（例如每个连接）调度请求，以便在服务器之间更好地进行负载均衡。当一台或多台服务器发生故障时，可以屏蔽故障。服务器管理变得越来越容易，管理员可以随时将一台或更多服务器带入和退出服务，这不会中断对最终用户的服务。

  负载平衡可以在两个级别完成，即**应用程序级**和 **IP 级**。例如，反向代理和 pWEB 是一种应用程序级负载平衡方法，用于构建可扩展的 Web 服务器。它们将 HTTP 请求转发到集群中的不同 Web 服务器，获取结果，然后将其返回给客户端。由于在应用程序级别处理 HTTP 请求和回复的开销很高，因此当服务器节点数量增加到 5 个或更多时，应用程序级负载均衡器将成为一个新的瓶颈，这取决于每个服务器的吞吐量。

  IP 负载均衡的开销很小，服务器节点的最大数量可以达到 25 个或最多 100 个。这就是 **IPVS**的设计目的。

# How？

虚拟服务器以三种方式实现。Linux**Director** （LVS控制器）中同时存在三种 IP 负载平衡技术（数据包**转发方法**）：

- 通过 [NAT](http://www.linuxvirtualserver.org/VS-NAT.html) 的虚拟服务器
- 通过 [IP 隧道](http://www.linuxvirtualserver.org/VS-IPTunneling.html)的虚拟服务器
- 通过[直接路由](http://www.linuxvirtualserver.org/VS-DRouting.html)（**DR**，Direct Routing）的虚拟服务器：更改数据包上的MAC地址并将数据包转发到realserver

（可参考《凤凰架构》4.5章节“负载均衡”）



在LVS中，**Director**实际作用类似于一个路由器，为LVS设置了路由表。这些表允许控制器将数据包转发到 LVS 服务的 realserver。



# 架构



![img](http://www.linuxvirtualserver.org/lvs_architecture.jpg)



包含三个部分：

- **负载均衡器**

  是整个集群系统的前端计算机，在一组服务器之间平衡来自客户端的请求，以便客户端认为所有服务都来自**单个 IP 地址(VIP)**。

  负载均衡器是服务器集群系统的**单一入口点**，可以通过运行**IPVS**实现 IP 负载均衡技术 ，也可以运行KTCPVS实现应用级负载均衡。

  - 使用IPVS时，要求所有服务器提供相同的服务和内容，负载均衡器根据指定的调度算法和每个服务器的负载，将新的客户端请求转发到服务器。无论选择哪个服务器，客户端都应获得相同的结果。

  - 使用 KTCPVS 时，服务器可以有不同的内容，负载均衡可以根据请求的内容将请求转发到不同的服务器。由于 KTCPVS 是在 Linux 内核内部实现的，因此中继数据的开销最小，因此仍然可以具有高吞吐量。

- **服务器集群**

  是一组运行实际网络服务（如 Web、邮件、FTP、DNS 和媒体服务）的服务器。

  服务器集群的节点数可以根据系统接收的负载进行更改。当所有服务器都过载时，可以添加更多新服务器来处理不断增加的工作负载。对于大多数 Internet 服务（如 Web），请求通常不高度相关，并且可以在不同的服务器上并行运行。因此，随着服务器集群节点数的增加，整体性能几乎可以线性扩容。

- **共享存储**

  为服务器提供共享存储空间，使服务器轻松拥有相同的内容并提供相同的服务。

  共享存储可以是数据库系统、网络文件系统或分布式文件系统。服务器节点需要动态更新的数据应该存储在基于数据的系统中，当服务器节点在数据库系统中并行读取或写入数据时，数据库系统可以保证并发数据访问的一致性。静态数据通常保存在 NFS 和 CIFS 等网络文件系统中，以便所有服务器节点可以共享数据。但是，单个网络文件系统的可扩展性有限，例如，单个 NFS/CIFS 只能支持 4 到 8 台服务器的数据访问。对于大型集群系统，可以使用分布式/集群文件系统进行共享存储，例如 GPFS、Coda 和 GFS，然后也可以根据系统需求扩展共享存储。

# 软件组成

- **IPVS**在Linux内核内实现传送层负载均衡

  IPVS基于**Netfilter**在 Linux 内核内实现传送层负载平衡，即所谓的第 4 层交换（实际IP在OSI第三层）。在主机上运行的IPVS充当真实服务器集群前端的负载平衡器，它可以将基于TCP/UDP的服务的请求定向到真实服务器，并使真实服务器的服务在单个IP地址上显示为虚拟服务。

  

- KTCPVS 在 Linux 内核内实现应用级负载均衡（第 7 层）

- TCPSP 为 Linux 内核实现 tcp 拼接。

- TCPHA 是 Linux 内核的开源 TCP 切换实现，由 Li Wang 编写。

# 参考资料

- 🔥 The [LVS-HOWTO and LVS-mini-HOWTO](http://www.austintek.com/LVS/LVS-HOWTO/) written by Joseph Mack.
- [The LVS/NAT working principle and configuration instructions.](http://www.linuxvirtualserver.org/VS-NAT.html)
- [The LVS/TUN working principle and configuration instructions.](http://www.linuxvirtualserver.org/VS-IPTunneling.html)
- [The LVS/DR working principle and configuration instructions.](http://www.linuxvirtualserver.org/VS-DRouting.html)
- [High availabilty issues of LVS](http://www.linuxvirtualserver.org/HighAvailability.html)
- [Job scheduling algorithms used in LVS](http://www.linuxvirtualserver.org/docs/scheduling.html)
- [A document about the local node feature](http://www.linuxvirtualserver.org/docs/LocalNode.html)
- [The arp problem of LVS/TUN and LVS/DR](http://www.linuxvirtualserver.org/docs/arp.html)
- [The persistence handling in LVS](http://www.linuxvirtualserver.org/docs/persistence.html)
- [LVS defense strategies against DoS attack](http://www.linuxvirtualserver.org/docs/defense.html)
- [IPVS connection synchronization](http://www.linuxvirtualserver.org/docs/sync.html)
- [IPVS sysctl variables](http://www.linuxvirtualserver.org/docs/sysctl.html)
- [Using LVS/TUN with FreeBSD and Solaris Real Servers](http://kb.linuxvirtualserver.org/wiki/LVS/TUN_mode_with_FreeBSD_and_Solaris_realserver)
- [Community contributed LVS knowledge base](http://kb.linuxvirtualserver.org/wiki/Main_Page) - wiki site



# 构建高可用的LVS集群

高可用性是一个很大的领域。一个先进的高可用性系统包括可靠的组通信子系统、成员管理、仲裁子系统、并发控制子系统等。一定有很多工作要做。但是，我们现在可以使用一些现有的软件包来构建高可用性的LVS集群系统。





![LVS high availability](http://www.linuxvirtualserver.org/lvs_ha.jpg)



- **对服务集群的健康监控**

通常，负载均衡器上运行服务**监视器守护程序（Service monitor）**以定期检查服务器运行状况，如 LVS 高可用性图所示。如果在指定时间内没有响应来自服务器的服务访问请求或 ICMP ECHO_REQUEST，则服务监视器将认为该服务器已失效，并将其从负载平衡器的可用服务器列表中删除，因此不会向该失效服务器发送新请求。当服务监视器检测到失效的服务器已恢复工作时，服务监视器会将该服务器添加回可用服务器列表。因此，负载均衡器可以自动屏蔽服务守护进程或服务器的故障。此外，管理员还可以使用系统工具添加新服务器以提高系统吞吐量或删除服务器进行系统维护，而不会关闭整个系统服务。



- **防止因LB导致的单点故障**

负载均衡器可能成为整个系统的单个故障点。为了防止整个系统因负载均衡器故障而停止服务，我们需要设置负载均衡器的备份（或多个备份）。两个心跳守护程序分别在主守护程序和备份守护程序上运行，它们定期通过串行线路和/或网络接口对消息进行心跳。当备份LB的守护进程在指定时间内未收到来自主节点的检测信号消息时，它将接管虚拟 IP 地址以提供负载均衡服务。当发生故障的负载均衡器重新工作时，有两种解决方案，一种是它自动成为备份负载均衡器，另一种是运行的LB主动释放 VIP 地址（**抢占式**），恢复的LB接管 VIP 地址并再次成为主负载均衡器。



- **连接状态同步**

主负载均衡器拥有连接的状态，即连接转发到哪个服务器。如果备份负载均衡器在没有这些连接信息的情况下接管，则客户端必须再次发送其请求以访问服务。**为了使负载均衡器故障转移对客户端应用程序透明，我们在 IPVS 中实现了连接同步**，主 IPVS 负载均衡器通过 UDP 多播将连接信息同步到备份负载均衡器。当备份负载均衡器在主负载均衡器发生故障后接管时，备份负载均衡器将具有大多数连接的状态，以便几乎所有连接都可以继续通过备份负载均衡器访问服务。

*此处不讨论数据库、网络文件系统或分布式文件系统的可用性。*



## 可用的实践

已有多个可用的软件包，通过与LVS结合提供高可用性。例如 Red Hat Piranha、**Keepalived**、UltraMonkey、heartbeat plus ldirectord 和 heartbeat plus mon。

## Keepalived解决方案

Keepalived 为 LVS 集群提供强大而健壮的运行状况检查。它实现了多层健康检查框架用于服务器故障转移 ，以及用于处理控制器（**Director**）故障转移的 VRRPv2 协议栈。

**示例**

使用 keepalived 来构建一个具有两个负载均衡器和三个 Web 服务器的高可用性 VS/NAT Web 集群。拓扑如下图所示。在示例中，虚拟 IP 地址和网关 IP 地址分别为 **10.23.8.80** 和 **172.18.1.254**，它们**浮动在两个负载均衡器（LD1 和 LD2）**之间，三个真实服务器的 IP 地址分别为 172.18.1.11、172.18.1.12 和 172.18.1.13。

![img](http://www.linuxvirtualserver.org/docs/ha/keepalived.jpg)

在我们的示例中，LD1 的 keepalived 配置文件 （/etc/keepalived/keepalived.conf） 如下所示：

```shell
vrrp_sync_group VG1 {
    group {
        VI_1
        VI_2
    }
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        10.23.8.80
    }
}

vrrp_instance VI_2 {
    state MASTER
    interface eth1
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        172.18.1.254
    }
}

virtual_server 10.23.8.80 80 {
    delay_loop 6
    lb_algo wlc
    lb_kind NAT
    persistence_timeout 600
    protocol TCP

    real_server 172.18.1.11 80 {
        weight 100
        TCP_CHECK {
            connect_timeout 3
        }
    }
    real_server 172.18.1.12 80 {
        weight 100
        TCP_CHECK {
            connect_timeout 3
        }
    }
    real_server 172.18.1.13 80 {
        weight 100
        TCP_CHECK {
            connect_timeout 3
        }
    }
}
```



LD2 的 Keepalived 配置文件与 LD1 类似，只是将 VI_1 和 VI_2 的状态从 MASTER 更改为 BACKUP。